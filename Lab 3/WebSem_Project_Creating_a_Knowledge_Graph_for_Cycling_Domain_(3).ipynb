{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpecDDB_GGLl"
      },
      "source": [
        "# WebSem Project: Constructing and Querying a Knowledge Graph in the Cycling Domain\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The goal of this project is to extract information from multilingual textual documents about cycling and create a knowledge graph (KG) using the extracted entities and relations. The KG will be compatible with a cycling ontology and queries will be written in SPARQL to retrieve specific information from the KG. The project will be implemented using Jupyter Notebook and the following steps will be followed:\n",
        "\n",
        "* Collect multilingual textual documents about cycling.\n",
        "* Pre-process the documents to get clean text files.\n",
        "* Run named entity recognition (NER) on the documents to extract named entities of the type Person, Organization and Location using spaCy.\n",
        "* Run co-reference resolution on the input text using spaCy.\n",
        "* Disambiguate the entities with Wikidata using OpenTapioca.\n",
        "* Run relation extraction using Stanford OpenIE.\n",
        "* Implement some mappings between the entity types and relations returned with the cycling ontology you developed during the Assignment 1 in order to create a knowledge graph of the domain represented in RDF.\n",
        "* Load the data in the Corese engine as you did for the Assignment 2 with your cycling ontology and the knowledge graph built in the previous step and write some SPARQL queries to retrieve specific information from the KG.\n",
        "\n",
        "### Useful resources\n",
        "* The github repository \"Building knowledge graph from input data\" at  https://github.com/varun196/knowledge_graph_from_unstructured_text can be used as an inspiration.\n",
        "\n",
        "### References\n",
        "* NLTK: https://www.nltk.org/\n",
        "* spaCy: https://spacy.io/\n",
        "* Stanford OpenIE: https://nlp.stanford.edu/software/openie.html\n",
        "* OpenTapioca: https://opentapioca.org/\n",
        "* Corese engine: https://project.inria.fr/corese/\n",
        "* Wikidata: https://www.wikidata.org/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzMinLFxGUFx"
      },
      "source": [
        "## Step 1: Collect multilingual textual documents about cycling\n",
        "For this mini project, we will collect multilingual textual documents about cycling from various sources such as news articles, blog posts, and race reports. We will download the documents and save them in a directory called `cycling_docs`.\n",
        "\n",
        "The list of documents to download are available at:\n",
        "\n",
        "* English:\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21\n",
        " - https://www.bbc.com/sport/cycling/61940037\n",
        " - https://www.bbc.com/sport/cycling/62017114 (stage 1)\n",
        " - https://www.bbc.com/sport/cycling/62097721 (stage 7)\n",
        " - https://www.bbc.com/sport/cycling/62153759 (stage 11)\n",
        " - https://www.bbc.co.uk/sport/cycling/62285420 (stage 21)\n",
        "\n",
        "* French:\n",
        " - https://fr.wikipedia.org/wiki/Tour_de_France_2022\n",
        " - https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html\n",
        " - https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HccrPk8uGVz3",
        "outputId": "c57b2ecf-3540-4e01-acd3-98b42146d4ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h2024-01-17 13:25:46.808447: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-17 13:25:46.808515: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-17 13:25:46.809928: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-17 13:25:46.818634: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-17 13:25:48.391869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.6.0/en_core_web_lg-3.6.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "2024-01-17 13:26:27.857282: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-01-17 13:26:27.857372: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-01-17 13:26:27.859582: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-01-17 13:26:27.873776: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-01-17 13:26:29.596520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting fr-core-news-lg==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_lg-3.6.0/fr_core_news_lg-3.6.0-py3-none-any.whl (571.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.8/571.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from fr-core-news-lg==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.10.3)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->fr-core-news-lg==3.6.0) (2.1.3)\n",
            "Installing collected packages: fr-core-news-lg\n",
            "Successfully installed fr-core-news-lg-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('fr_core_news_lg')\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pycorenlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "#\n",
        "# Feel free to install more dependencies if needed!\n",
        "#\n",
        "\n",
        "# Install jusText for automatically extracting text from web pages\n",
        "!pip install --quiet jusText\n",
        "\n",
        "# Install nltk for text processing\n",
        "!pip install --quiet nltk\n",
        "\n",
        "# Install spaCy for NER extraction\n",
        "!pip install --quiet spacy\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download fr_core_news_lg\n",
        "\n",
        "# Install pycorenlp for Stanford CoreNLP\n",
        "!pip install --quiet pycorenlp\n",
        "\n",
        "# Install pandas for data visualization\n",
        "!pip install --quiet pandas\n",
        "\n",
        "# Install rdflib for writing RDF\n",
        "!pip install --quiet rdflib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_031XjozIHqs",
        "outputId": "6cad1bd8-75c4-4ec8-8519-2d83e9b83d47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded https://en.wikipedia.org/wiki/2022_Tour_de_France into cycling_docs/english/2022_Tour_de_France.txt\n",
            "Downloaded https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11 into cycling_docs/english/2022_Tour_de_France,_Stage_1_to_Stage_11.txt\n",
            "Downloaded https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21 into cycling_docs/english/2022_Tour_de_France,_Stage_12_to_Stage_21.txt\n",
            "Downloaded https://www.bbc.com/sport/cycling/61940037 into cycling_docs/english/61940037.txt\n",
            "Downloaded https://www.bbc.com/sport/cycling/62017114 into cycling_docs/english/62017114.txt\n",
            "Downloaded https://www.bbc.com/sport/cycling/62097721 into cycling_docs/english/62097721.txt\n",
            "Downloaded https://www.bbc.com/sport/cycling/62153759 into cycling_docs/english/62153759.txt\n",
            "Downloaded https://www.bbc.co.uk/sport/cycling/62285420 into cycling_docs/english/62285420.txt\n",
            "Downloaded https://fr.wikipedia.org/wiki/Tour_de_France_2022 into cycling_docs/french/Tour_de_France_2022.txt\n",
            "Downloaded https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html into cycling_docs/french/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html.txt\n",
            "Downloaded https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html into cycling_docs/french/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html.txt\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "import requests\n",
        "import justext\n",
        "import os\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "\n",
        "# Define a function to get filename from URL\n",
        "def get_filename_from_url(url):\n",
        "  urlpath = urlsplit(url).path\n",
        "  return os.path.basename(urlpath)\n",
        "\n",
        "\n",
        "# Define a function to download URLs and extract text\n",
        "def download_urls(urls_list, language):\n",
        "  # Loop over each URL in the list\n",
        "  for url in urls_list:\n",
        "    # Fetch and extract text from the URL using jusText\n",
        "    response = requests.get(url)\n",
        "    paragraphs = justext.justext(\n",
        "      response.content,\n",
        "      justext.get_stoplist(language.capitalize()),\n",
        "      no_headings=True,\n",
        "      max_heading_distance=150,\n",
        "      length_low=70,\n",
        "      length_high=140,\n",
        "      stopwords_low=0.2,\n",
        "      stopwords_high=0.3,\n",
        "      max_link_density=0.4\n",
        "    )\n",
        "    extracted_text = '\\n'.join(list(filter(None, map(\n",
        "      lambda paragraph: paragraph.text if not paragraph.is_boilerplate else '',\n",
        "      paragraphs\n",
        "    ))))\n",
        "\n",
        "    # Truncate text if it's too long\n",
        "    extracted_text = extracted_text[0:10000]\n",
        "\n",
        "    # Create the output directory if it does not exist\n",
        "    output_dir = os.path.join('cycling_docs', language)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save extracted text as a .txt file\n",
        "    filename = get_filename_from_url(url)\n",
        "    output_path = os.path.join(output_dir, f'{filename}.txt')\n",
        "    with open(output_path, 'w') as f:\n",
        "      f.write(extracted_text)\n",
        "\n",
        "    print(f'Downloaded {url} into {output_path}')\n",
        "\n",
        "\n",
        "# List of URLs to download\n",
        "urls_list_english = [\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France',\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11',\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21',\n",
        "  'https://www.bbc.com/sport/cycling/61940037',\n",
        "  'https://www.bbc.com/sport/cycling/62017114',\n",
        "  'https://www.bbc.com/sport/cycling/62097721',\n",
        "  'https://www.bbc.com/sport/cycling/62153759',\n",
        "  'https://www.bbc.co.uk/sport/cycling/62285420',\n",
        "]\n",
        "urls_list_french = [\n",
        "  'https://fr.wikipedia.org/wiki/Tour_de_France_2022',\n",
        "  'https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html',\n",
        "  'https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html',\n",
        "]\n",
        "\n",
        "# Download the listed URLs\n",
        "download_urls(urls_list_english, 'english')\n",
        "download_urls(urls_list_french, 'french')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wg52A5ELGWvQ"
      },
      "source": [
        "## Step 2: Pre-process the documents to get clean txt files\n",
        "We will pre-process the documents to get clean txt files by removing any unnecessary characters, punctuation, and stopwords. We will use Python's [re](https://docs.python.org/3/library/re.html) and [nltk](https://www.nltk.org/) libraries for this purpose. We will save the results in a `clean_docs` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t9sdmals1W7w"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Document class which holds all the necessary variables for the purpose of this\n",
        "project.\n",
        "\"\"\"\n",
        "class Document:\n",
        "  def __init__(self, text, language = None, raw_text = None, filepath = None):\n",
        "    self.language = language   # Language of the document\n",
        "    self.raw_text = raw_text   # Origial text before cleaning\n",
        "    self.text = text           # Text after cleaning\n",
        "    self.resolved_text = None  # Text after resolving co-references\n",
        "    self.filepath = filepath   # Path to the document file\n",
        "    self.spacy_entities = []   # List of spaCy entities\n",
        "    self.coreferences = None   # CoreNLP coreferences object\n",
        "    self.wiki_entities = {}    # Dictionary of Wikidata entities\n",
        "    self.relations = []        # List of OpenIE relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9JsLAGsxsyI",
        "outputId": "232f5fa7-65fb-4e93-a25b-e21c349ae7c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# 📝 TODO: Import the necessary libraries for natural language processing\n",
        "import re\n",
        "import nltk\n",
        "import os\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def clean_text(dirty_text, language):\n",
        "  # 📝 TODO: Define a function to clean text (words tokenization, stopwords\n",
        "  #          removal, ...).\n",
        "  # `cleaned_text = ...`\n",
        "\n",
        "  # Create list with stop words\n",
        "  stop_words = set(nltk.corpus.stopwords.words(language))\n",
        "\n",
        "  # Convert text to lower case\n",
        "  cleaned_text = dirty_text.lower()\n",
        "\n",
        "  # Remove puntuation marks, spaces and all non necessary characters\n",
        "  cleaned_text = re.sub(\"\\[[^\\]]*\\]\", \" \", cleaned_text) # Remove annotations like \"[2]\"\n",
        "  cleaned_text = re.sub(\"[^a-zA-Z0-9À-ÖØ-öø-ÿ']\", \" \", cleaned_text)\n",
        "  cleaned_text = nltk.tokenize.word_tokenize(cleaned_text)\n",
        "\n",
        "  # Remove all non essential words\n",
        "  cleaned_text = [w for w in cleaned_text if not w in stop_words]\n",
        "\n",
        "  # Return the cleaned text\n",
        "  return \" \".join(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "KhFqE5TleLz5"
      },
      "outputs": [],
      "source": [
        "# Define a function to process a file and write the result to a new file\n",
        "def process_file(file, language):\n",
        "  # Open the file in read-only mode and read all of its lines\n",
        "  with open(file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  # Concatenate all the lines into a single string\n",
        "  raw_text = '\\n'.join(lines)\n",
        "\n",
        "  # Clean the text using the `clean_text` function\n",
        "  cleaned_text = clean_text(raw_text, language)\n",
        "\n",
        "  # Create a new document and return it\n",
        "  doc = Document(cleaned_text, language=language, raw_text=raw_text, filepath=os.path.abspath(file))\n",
        "  return doc\n",
        "\n",
        "\n",
        "# Create a list to store all our documents\n",
        "docs = []\n",
        "\n",
        "# Loop through all the files in the \"cycling_docs\" folder\n",
        "folder = 'cycling_docs'\n",
        "for language in os.listdir(folder):\n",
        "  for filename in os.listdir(os.path.join(folder, language)):\n",
        "    # Construct the full path to the file\n",
        "    file = os.path.join(folder, language, filename)\n",
        "\n",
        "    # Check if the file is a regular file and has a .txt extension\n",
        "    if os.path.isfile(file) and file.endswith('.txt'):\n",
        "      # Process the file and append the new document to our list\n",
        "      doc = process_file(file, language)\n",
        "      docs.append(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "yi3wcREO1plh",
        "outputId": "bdcacc13-b740-4ad2-f4a9-298c4d41c71e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"yves lampaert stage one tour de france defending champion tadej pogacar took time main rivals opening individual time trial copenhagen lampaert rode superbly negotiate damp conditions finish five seconds clear fellow belgian wout van aert pogacar two seconds back third place nine seconds ahead slovenian compatriot primoz roglic britain 's adam yates geraint thomas finished 13th 18th fellow ineos grenadiers rider tom pidcock sandwiched pair taking advantage favourable later conditions roll line 15th thomas ' mixed fortunes thomas 36 tour 2018 rode stage gilet forgetting take start lost 18 seconds first part flat technical route around danish capital worst first half time trial ever done thomas said wanted start fairly conservatively power wise everyone telling go easy corners 's three weeks crash first corners cornered like wife n't ridden bike 12 years unbelievable realised still gilet went first time check 18 seconds took pin know could done better ride annoying ' 'm farmer 's son belgium ' welshman 's time far disastrous another day may closer favourites stage ineos team mate world time trial champion filippo ganna bettered mathieu van der poel 's early mark seeing van aert pogacar immediately eclipse heavy rain began subside quick step alpha vinyl 's lampaert delivered stunning performance claim surprise victory 31 year old disqualified tour belgium barging tim wellens two weeks ago tears end first tour victory confirmed mind exploding lampaert said thought top 10 would great beat best riders world 'm farmer 's son belgium never expected roads really wet pot holes full water think conditions main favourites always corners said 'yves go faster trust tyres '\""
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Display the text of the first document\n",
        "display(docs[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXH__c_rGaIo"
      },
      "source": [
        "## Step 3: Run named entity recognition (NER) on the documents\n",
        "We will use [spaCy](https://spacy.io)'s pre-trained models to perform NER on the documents and extract the entities of type PER/ORG/LOC. We will save the extracted entities in a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "MkMYXqVFGy9t",
        "outputId": "df680c87-9751-42fa-ed39-5833e68cb404"
      },
      "outputs": [],
      "source": [
        "# 📝 TODO: Import spaCy and other libraries that might be required for entity\n",
        "#          extraction\n",
        "import spacy\n",
        "\n",
        "def extract_entities(text, language):\n",
        "  # 📝 TODO: Use spaCy to extract named entities and store them into a list.\n",
        "  # The format of the end result should look like this:\n",
        "  # ```\n",
        "  # entities = [\n",
        "  #   { \"text\": \"Tour de France\", \"label\": \"ORG\" },\n",
        "  #   { \"text\": \"Peter Sagan\", \"label\": \"PERSON\" },\n",
        "  # ]\n",
        "  # ```\n",
        "  if language == \"english\":\n",
        "    nlp = spacy.load('en_core_web_lg')\n",
        "  elif language == \"french\":\n",
        "    nlp = spacy.load('fr_core_news_lg')\n",
        "  else:\n",
        "    raise Exception(\"Language not supported!\")\n",
        "\n",
        "  doc = nlp(text)\n",
        "  entities = []\n",
        "  for entity in doc.ents:\n",
        "    entities.append({\n",
        "        \"text\":entity.text,\n",
        "        \"label\":entity.label_\n",
        "    })\n",
        "\n",
        "  # Return extracted entities\n",
        "  return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REq50-k7ePSf"
      },
      "outputs": [],
      "source": [
        "# Extract entities for each document\n",
        "for doc in docs:\n",
        "  doc.spacy_entities = extract_entities(doc.text, doc.language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkFvU6OfDwux"
      },
      "source": [
        "Display entities which have been extracted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ER2xsq4WYJgp"
      },
      "outputs": [],
      "source": [
        "# 📝 TODO: Display the extracted entities for the first document\n",
        "for entity in docs[0].spacy_entities:\n",
        "  print(entity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAsxblm4GceJ"
      },
      "source": [
        "## Step 4: Run co-reference resolution on the input text\n",
        "We will use CoreNLP to perform [co-reference resolution](https://en.wikipedia.org/wiki/Coreference) on the input text and resolve coreferences.\n",
        "\n",
        "For this project, we will use a hosted version of CoreNLP at: https://corenlp.tools.eurecom.fr/ (username: `websem`, password: `eurecom`). Feel free to try out the web interface before writing the code.\n",
        "\n",
        "First, we compute the annotations and store them into the `coreferences` variable of our Document:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0WOTNW3Ggg5"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "\n",
        "# Set up the CoreNLP client\n",
        "nlp = StanfordCoreNLP('https://websem:eurecom@corenlp.tools.eurecom.fr')\n",
        "\n",
        "# Define a function which computes coreferences for a given text and language\n",
        "def compute_coreferences(text, language):\n",
        "  props = {\n",
        "    'timeout': 300000,\n",
        "    'annotators': 'tokenize,ssplit,coref',\n",
        "    'pipelineLanguage': language[:2],\n",
        "    'outputFormat': 'json'\n",
        "  }\n",
        "\n",
        "  # Annotate the text for co-reference resolution\n",
        "  corenlp_output = nlp.annotate(text, properties=props)\n",
        "  try:\n",
        "    corenlp_output = json.loads(corenlp_output)\n",
        "  except Exception as err:\n",
        "    print(f'Unexpected response: {corenlp_output}')\n",
        "    raise\n",
        "\n",
        "  return corenlp_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajQ2T6QyhWop",
        "outputId": "07423e55-6dab-468a-cab0-d6b6f8852fe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"sentences\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"basicDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \"engineer\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"John\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"det\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"a\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"compound\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"software\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 6,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"enhancedDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \"engineer\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"John\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"det\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"a\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"compound\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"software\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 6,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"enhancedPlusPlusDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \"engineer\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"John\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"det\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"a\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"compound\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"software\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 5,\n",
            "          \"governorGloss\": \"engineer\",\n",
            "          \"dependent\": 6,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"entitymentions\": [\n",
            "        {\n",
            "          \"docTokenBegin\": 0,\n",
            "          \"docTokenEnd\": 1,\n",
            "          \"tokenBegin\": 0,\n",
            "          \"tokenEnd\": 1,\n",
            "          \"text\": \"John\",\n",
            "          \"characterOffsetBegin\": 0,\n",
            "          \"characterOffsetEnd\": 4,\n",
            "          \"ner\": \"PERSON\"\n",
            "        },\n",
            "        {\n",
            "          \"docTokenBegin\": 3,\n",
            "          \"docTokenEnd\": 5,\n",
            "          \"tokenBegin\": 3,\n",
            "          \"tokenEnd\": 5,\n",
            "          \"text\": \"software engineer\",\n",
            "          \"characterOffsetBegin\": 10,\n",
            "          \"characterOffsetEnd\": 27,\n",
            "          \"ner\": \"TITLE\"\n",
            "        }\n",
            "      ],\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"John\",\n",
            "          \"originalText\": \"John\",\n",
            "          \"lemma\": \"John\",\n",
            "          \"characterOffsetBegin\": 0,\n",
            "          \"characterOffsetEnd\": 4,\n",
            "          \"pos\": \"NNP\",\n",
            "          \"ner\": \"PERSON\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"is\",\n",
            "          \"originalText\": \"is\",\n",
            "          \"lemma\": \"be\",\n",
            "          \"characterOffsetBegin\": 5,\n",
            "          \"characterOffsetEnd\": 7,\n",
            "          \"pos\": \"VBZ\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"lemma\": \"a\",\n",
            "          \"characterOffsetBegin\": 8,\n",
            "          \"characterOffsetEnd\": 9,\n",
            "          \"pos\": \"DT\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"software\",\n",
            "          \"originalText\": \"software\",\n",
            "          \"lemma\": \"software\",\n",
            "          \"characterOffsetBegin\": 10,\n",
            "          \"characterOffsetEnd\": 18,\n",
            "          \"pos\": \"NN\",\n",
            "          \"ner\": \"TITLE\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \"engineer\",\n",
            "          \"originalText\": \"engineer\",\n",
            "          \"lemma\": \"engineer\",\n",
            "          \"characterOffsetBegin\": 19,\n",
            "          \"characterOffsetEnd\": 27,\n",
            "          \"pos\": \"NN\",\n",
            "          \"ner\": \"TITLE\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 6,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"lemma\": \".\",\n",
            "          \"characterOffsetBegin\": 27,\n",
            "          \"characterOffsetEnd\": 28,\n",
            "          \"pos\": \".\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 1,\n",
            "      \"basicDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"talented\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"He\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"advmod\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"very\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"enhancedDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"talented\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"He\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"advmod\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"very\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"enhancedPlusPlusDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"talented\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"He\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"advmod\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"very\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"talented\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"entitymentions\": [\n",
            "        {\n",
            "          \"docTokenBegin\": 6,\n",
            "          \"docTokenEnd\": 7,\n",
            "          \"tokenBegin\": 0,\n",
            "          \"tokenEnd\": 1,\n",
            "          \"text\": \"He\",\n",
            "          \"characterOffsetBegin\": 29,\n",
            "          \"characterOffsetEnd\": 31,\n",
            "          \"ner\": \"PERSON\"\n",
            "        }\n",
            "      ],\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"He\",\n",
            "          \"originalText\": \"He\",\n",
            "          \"lemma\": \"he\",\n",
            "          \"characterOffsetBegin\": 29,\n",
            "          \"characterOffsetEnd\": 31,\n",
            "          \"pos\": \"PRP\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"is\",\n",
            "          \"originalText\": \"is\",\n",
            "          \"lemma\": \"be\",\n",
            "          \"characterOffsetBegin\": 32,\n",
            "          \"characterOffsetEnd\": 34,\n",
            "          \"pos\": \"VBZ\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"very\",\n",
            "          \"originalText\": \"very\",\n",
            "          \"lemma\": \"very\",\n",
            "          \"characterOffsetBegin\": 35,\n",
            "          \"characterOffsetEnd\": 39,\n",
            "          \"pos\": \"RB\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"talented\",\n",
            "          \"originalText\": \"talented\",\n",
            "          \"lemma\": \"talented\",\n",
            "          \"characterOffsetBegin\": 40,\n",
            "          \"characterOffsetEnd\": 48,\n",
            "          \"pos\": \"JJ\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"lemma\": \".\",\n",
            "          \"characterOffsetBegin\": 48,\n",
            "          \"characterOffsetEnd\": 49,\n",
            "          \"pos\": \".\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 2,\n",
            "      \"basicDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"designer\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"Sarah\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"det\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"a\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"enhancedDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"designer\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"Sarah\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"det\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"a\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"enhancedPlusPlusDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"designer\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"Sarah\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"cop\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"is\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"det\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"a\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"designer\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"entitymentions\": [\n",
            "        {\n",
            "          \"docTokenBegin\": 11,\n",
            "          \"docTokenEnd\": 12,\n",
            "          \"tokenBegin\": 0,\n",
            "          \"tokenEnd\": 1,\n",
            "          \"text\": \"Sarah\",\n",
            "          \"characterOffsetBegin\": 50,\n",
            "          \"characterOffsetEnd\": 55,\n",
            "          \"ner\": \"PERSON\"\n",
            "        },\n",
            "        {\n",
            "          \"docTokenBegin\": 14,\n",
            "          \"docTokenEnd\": 15,\n",
            "          \"tokenBegin\": 3,\n",
            "          \"tokenEnd\": 4,\n",
            "          \"text\": \"designer\",\n",
            "          \"characterOffsetBegin\": 61,\n",
            "          \"characterOffsetEnd\": 69,\n",
            "          \"ner\": \"TITLE\"\n",
            "        }\n",
            "      ],\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"Sarah\",\n",
            "          \"originalText\": \"Sarah\",\n",
            "          \"lemma\": \"Sarah\",\n",
            "          \"characterOffsetBegin\": 50,\n",
            "          \"characterOffsetEnd\": 55,\n",
            "          \"pos\": \"NNP\",\n",
            "          \"ner\": \"PERSON\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"is\",\n",
            "          \"originalText\": \"is\",\n",
            "          \"lemma\": \"be\",\n",
            "          \"characterOffsetBegin\": 56,\n",
            "          \"characterOffsetEnd\": 58,\n",
            "          \"pos\": \"VBZ\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"a\",\n",
            "          \"originalText\": \"a\",\n",
            "          \"lemma\": \"a\",\n",
            "          \"characterOffsetBegin\": 59,\n",
            "          \"characterOffsetEnd\": 60,\n",
            "          \"pos\": \"DT\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"designer\",\n",
            "          \"originalText\": \"designer\",\n",
            "          \"lemma\": \"designer\",\n",
            "          \"characterOffsetBegin\": 61,\n",
            "          \"characterOffsetEnd\": 69,\n",
            "          \"pos\": \"NN\",\n",
            "          \"ner\": \"TITLE\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"lemma\": \".\",\n",
            "          \"characterOffsetBegin\": 69,\n",
            "          \"characterOffsetEnd\": 70,\n",
            "          \"pos\": \".\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \"\",\n",
            "          \"after\": \" \"\n",
            "        }\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"index\": 3,\n",
            "      \"basicDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"works\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"She\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"case\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"him\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"with\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nmod\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"him\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"enhancedDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"works\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"She\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"case\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"him\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"with\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nmod:with\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"him\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"enhancedPlusPlusDependencies\": [\n",
            "        {\n",
            "          \"dep\": \"ROOT\",\n",
            "          \"governor\": 0,\n",
            "          \"governorGloss\": \"ROOT\",\n",
            "          \"dependent\": 2,\n",
            "          \"dependentGloss\": \"works\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nsubj\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 1,\n",
            "          \"dependentGloss\": \"She\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"case\",\n",
            "          \"governor\": 4,\n",
            "          \"governorGloss\": \"him\",\n",
            "          \"dependent\": 3,\n",
            "          \"dependentGloss\": \"with\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"nmod:with\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 4,\n",
            "          \"dependentGloss\": \"him\"\n",
            "        },\n",
            "        {\n",
            "          \"dep\": \"punct\",\n",
            "          \"governor\": 2,\n",
            "          \"governorGloss\": \"works\",\n",
            "          \"dependent\": 5,\n",
            "          \"dependentGloss\": \".\"\n",
            "        }\n",
            "      ],\n",
            "      \"entitymentions\": [\n",
            "        {\n",
            "          \"docTokenBegin\": 16,\n",
            "          \"docTokenEnd\": 17,\n",
            "          \"tokenBegin\": 0,\n",
            "          \"tokenEnd\": 1,\n",
            "          \"text\": \"She\",\n",
            "          \"characterOffsetBegin\": 71,\n",
            "          \"characterOffsetEnd\": 74,\n",
            "          \"ner\": \"PERSON\"\n",
            "        },\n",
            "        {\n",
            "          \"docTokenBegin\": 19,\n",
            "          \"docTokenEnd\": 20,\n",
            "          \"tokenBegin\": 3,\n",
            "          \"tokenEnd\": 4,\n",
            "          \"text\": \"him\",\n",
            "          \"characterOffsetBegin\": 86,\n",
            "          \"characterOffsetEnd\": 89,\n",
            "          \"ner\": \"PERSON\"\n",
            "        }\n",
            "      ],\n",
            "      \"tokens\": [\n",
            "        {\n",
            "          \"index\": 1,\n",
            "          \"word\": \"She\",\n",
            "          \"originalText\": \"She\",\n",
            "          \"lemma\": \"she\",\n",
            "          \"characterOffsetBegin\": 71,\n",
            "          \"characterOffsetEnd\": 74,\n",
            "          \"pos\": \"PRP\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 2,\n",
            "          \"word\": \"works\",\n",
            "          \"originalText\": \"works\",\n",
            "          \"lemma\": \"work\",\n",
            "          \"characterOffsetBegin\": 75,\n",
            "          \"characterOffsetEnd\": 80,\n",
            "          \"pos\": \"VBZ\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 3,\n",
            "          \"word\": \"with\",\n",
            "          \"originalText\": \"with\",\n",
            "          \"lemma\": \"with\",\n",
            "          \"characterOffsetBegin\": 81,\n",
            "          \"characterOffsetEnd\": 85,\n",
            "          \"pos\": \"IN\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \" \"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 4,\n",
            "          \"word\": \"him\",\n",
            "          \"originalText\": \"him\",\n",
            "          \"lemma\": \"he\",\n",
            "          \"characterOffsetBegin\": 86,\n",
            "          \"characterOffsetEnd\": 89,\n",
            "          \"pos\": \"PRP\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \" \",\n",
            "          \"after\": \"\"\n",
            "        },\n",
            "        {\n",
            "          \"index\": 5,\n",
            "          \"word\": \".\",\n",
            "          \"originalText\": \".\",\n",
            "          \"lemma\": \".\",\n",
            "          \"characterOffsetBegin\": 89,\n",
            "          \"characterOffsetEnd\": 90,\n",
            "          \"pos\": \".\",\n",
            "          \"ner\": \"O\",\n",
            "          \"speaker\": \"PER0\",\n",
            "          \"before\": \"\",\n",
            "          \"after\": \"\"\n",
            "        }\n",
            "      ]\n",
            "    }\n",
            "  ],\n",
            "  \"corefs\": {\n",
            "    \"5\": [\n",
            "      {\n",
            "        \"id\": 3,\n",
            "        \"text\": \"Sarah\",\n",
            "        \"type\": \"PROPER\",\n",
            "        \"number\": \"SINGULAR\",\n",
            "        \"gender\": \"FEMALE\",\n",
            "        \"animacy\": \"ANIMATE\",\n",
            "        \"startIndex\": 1,\n",
            "        \"endIndex\": 2,\n",
            "        \"headIndex\": 1,\n",
            "        \"sentNum\": 3,\n",
            "        \"position\": [\n",
            "          3,\n",
            "          1\n",
            "        ],\n",
            "        \"isRepresentativeMention\": true\n",
            "      },\n",
            "      {\n",
            "        \"id\": 5,\n",
            "        \"text\": \"She\",\n",
            "        \"type\": \"PRONOMINAL\",\n",
            "        \"number\": \"SINGULAR\",\n",
            "        \"gender\": \"FEMALE\",\n",
            "        \"animacy\": \"ANIMATE\",\n",
            "        \"startIndex\": 1,\n",
            "        \"endIndex\": 2,\n",
            "        \"headIndex\": 1,\n",
            "        \"sentNum\": 4,\n",
            "        \"position\": [\n",
            "          4,\n",
            "          1\n",
            "        ],\n",
            "        \"isRepresentativeMention\": false\n",
            "      }\n",
            "    ],\n",
            "    \"6\": [\n",
            "      {\n",
            "        \"id\": 0,\n",
            "        \"text\": \"John\",\n",
            "        \"type\": \"PROPER\",\n",
            "        \"number\": \"SINGULAR\",\n",
            "        \"gender\": \"MALE\",\n",
            "        \"animacy\": \"ANIMATE\",\n",
            "        \"startIndex\": 1,\n",
            "        \"endIndex\": 2,\n",
            "        \"headIndex\": 1,\n",
            "        \"sentNum\": 1,\n",
            "        \"position\": [\n",
            "          1,\n",
            "          1\n",
            "        ],\n",
            "        \"isRepresentativeMention\": true\n",
            "      },\n",
            "      {\n",
            "        \"id\": 2,\n",
            "        \"text\": \"He\",\n",
            "        \"type\": \"PRONOMINAL\",\n",
            "        \"number\": \"SINGULAR\",\n",
            "        \"gender\": \"MALE\",\n",
            "        \"animacy\": \"ANIMATE\",\n",
            "        \"startIndex\": 1,\n",
            "        \"endIndex\": 2,\n",
            "        \"headIndex\": 1,\n",
            "        \"sentNum\": 2,\n",
            "        \"position\": [\n",
            "          2,\n",
            "          1\n",
            "        ],\n",
            "        \"isRepresentativeMention\": false\n",
            "      },\n",
            "      {\n",
            "        \"id\": 6,\n",
            "        \"text\": \"him\",\n",
            "        \"type\": \"PRONOMINAL\",\n",
            "        \"number\": \"SINGULAR\",\n",
            "        \"gender\": \"MALE\",\n",
            "        \"animacy\": \"ANIMATE\",\n",
            "        \"startIndex\": 4,\n",
            "        \"endIndex\": 5,\n",
            "        \"headIndex\": 4,\n",
            "        \"sentNum\": 4,\n",
            "        \"position\": [\n",
            "          4,\n",
            "          2\n",
            "        ],\n",
            "        \"isRepresentativeMention\": false\n",
            "      }\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Test co-references computation\n",
        "example = compute_coreferences(\"John is a software engineer. He is very talented. Sarah is a designer. She works with him.\", language=\"en\")\n",
        "\n",
        "# Pretty-print them\n",
        "print(json.dumps(example, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSnVwbZkhUii"
      },
      "outputs": [],
      "source": [
        "# Compute co-references for all documents\n",
        "for doc in docs:\n",
        "  if doc.language == \"english\":  # CoreNLP Coref-resolution only supports english\n",
        "    doc.coreferences = compute_coreferences(doc.raw_text, doc.language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBqJ8DTtoFpN"
      },
      "source": [
        "The first step is to display all co-references for each mentions in the text.\n",
        "\n",
        "For example:\n",
        "\n",
        "> \"He\" -> \"John\"\n",
        ">\n",
        "> \"She\" -> \"Sarah\"\n",
        ">\n",
        "> \"him\" -> \"John\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp2BHxDHoSUd",
        "outputId": "e98945d2-1806-4e97-cdf2-4fd054d87c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"She\" -> \"Sarah\"\n",
            "\"He\" -> \"John\"\n",
            "\"him\" -> \"John\"\n"
          ]
        }
      ],
      "source": [
        "for coref_cluster in example['corefs'].values():\n",
        "  # 📝 TODO: Print each co-references like so: \"He\" -> \"John\"\n",
        "  # 💡 Each cluster has one representative mention, flagged with `isRepresentativeMention: True`\n",
        "\n",
        "  # Find first the representative mention\n",
        "  representativeMention = None\n",
        "  for r in coref_cluster:\n",
        "    if r['isRepresentativeMention']:\n",
        "      representativeMention = r\n",
        "      break\n",
        "\n",
        "  # For all the non representative mention we print them like: \"him\" -> \"John\"\n",
        "  for r in coref_cluster:\n",
        "    if not r['isRepresentativeMention']:\n",
        "      print(\"\\\"{}\\\" -> \\\"{}\\\"\".format(r['text'], representativeMention['text']))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVhTqao-5Z55"
      },
      "source": [
        "### 🏆 Challenge\n",
        "\n",
        "Replace values within the text with their resolved co-reference. For example, with the following text:\n",
        "\n",
        "> **John** is a software engineer. **He** is very talented.\n",
        "\n",
        "In the second sentence, the pronoun \"He\" would be replaced with its co-reference, and the final text would become:\n",
        "\n",
        "> **John** is a software engineer. **John** is very talented."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4P_mgqCeebD"
      },
      "outputs": [],
      "source": [
        "# Define a function which resolves coreferences inside a document\n",
        "def resolve_coreferences(raw_text, corefs):\n",
        "  corenlp_output = corefs['corefs']\n",
        "  resolved_text = raw_text\n",
        "\n",
        "  # 📝 TODO: Replace values within the text with their resolved co-reference.\n",
        "  # 💡 You can start by printing the `corenlp_output` object to understand its\n",
        "  #    structure.\n",
        "\n",
        "  # Build a dictionary with the strings we are going to replace and their replacement (representative mention)\n",
        "  replacements = {}\n",
        "  for coref_cluster in corenlp_output.values():\n",
        "    # Find first the representative mention\n",
        "    representativeMention = None\n",
        "    for r in coref_cluster:\n",
        "      if r['isRepresentativeMention']:\n",
        "        representativeMention = r\n",
        "        break\n",
        "    # For all the non representative mention include them as keys in the replacements dictionary\n",
        "    for r in coref_cluster:\n",
        "      if not r['isRepresentativeMention']:\n",
        "        replacements[r['text']] =  representativeMention['text']\n",
        "\n",
        "  # Replace all non representative mentions!\n",
        "  for key, value in replacements.items():\n",
        "    resolved_text = resolved_text.replace(\" \" + key + \" \", \" \" + value +  \" \") # The key must be surrounded by spaces in order to be an independent word. EX: Can replace all \"he\" instead of \" he \"\n",
        "    resolved_text = resolved_text.replace(\" \" + key + \".\", \" \" + value + \".\") # Or end with a period\n",
        "    resolved_text = resolved_text.replace(\" \" + key + \",\", \" \" + value + \",\") # or end with a coma\n",
        "    resolved_text = resolved_text.replace(\" \" + key + \"'\", \" \" + value + \"'\") # or end with a '\n",
        "\n",
        "  return resolved_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rXXcAtMklMk",
        "outputId": "243cd094-d951-40b6-a5dc-4f372852996b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "John is a software engineer. He is very talented. Sarah is a designer. She works with him.\n",
            "John is a software engineer. John is very talented. Sarah is a designer. Sarah works with John.\n"
          ]
        }
      ],
      "source": [
        "# Test resolving co-references\n",
        "original_text = \"John is a software engineer. He is very talented. Sarah is a designer. She works with him.\"\n",
        "corefs = compute_coreferences(original_text, language=\"en\")\n",
        "resolved_text = resolve_coreferences(original_text, corefs)\n",
        "print(original_text)\n",
        "print(resolved_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntDsoAUikd4U"
      },
      "outputs": [],
      "source": [
        "# Resolve co-references for all documents\n",
        "for doc in docs:\n",
        "  if doc.coreferences is not None:\n",
        "    doc.resolved_text = resolve_coreferences(doc.raw_text, doc.coreferences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hINiwG5V-__O",
        "outputId": "1dab6b85-fdfa-437b-e32b-85aa2d0a7601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The 2022 Tour de France is the 109th edition of The 2022 Tour de France. a furious fight for the break 's started in Copenhagen, Denmark on 1 July[1] and ended with the final stage at Champs-Élysées, Paris on 24 July.[2]\n",
            "\n",
            "The twelfth stage featured the race 's's queen stage as the riders travelled from Briançon to Alpe d'Huez. the riders gradually climbed from the get-go, passing through the intermediate sprint in Le Monêtier-les-Bains after 11.8 kilometres (7.3 mi). Immediately afterwards, the riders made Team Jumbo -- Visma proceeded to block the road and allowed the break 's to extend their advantage to more than 14 minutes , ensuring that the break 's will fight for the race 's queen stage win way back up the Col du Galibier but this time, the riders went up the riders categorie Col du Lautaret side, which is 23 kilometres (14 mi) long with an average of 5.1 percent. After descending the Galibier and the Télégraphe, the riders made Team Jumbo -- Visma proceeded to block the road and allowed the break 's to extend their advantage to more than 14 minutes , ensuring that the break 's will fight for the race 's queen stage win way towards the riders categorieCol de la Col de la Croix de Fer, a 29-kilometre (18 mi) climb with an average of 5.2 percent. Despite the average gradient, the climb , Oliveira , Goossens , Schönberger , and Perez had two short descents while the last 6.5 kilometres (4.0 mi) averaged 8.3 percent. At the top, there were 54.5 kilometres (33.9 mi) left on the race 's queen stage. Following a step descent and a flat section, the riders reached the foot of the riders categorie Alpe d'Huez for another summit finish. the climb , Oliveira , Goossens , Schönberger , and Perez is 13.8 kilometres (8.6 mi) long with an average of 8.1 percent. The first 11 kilometres (6.8 mi) of the climb , Oliveira , Goossens , Schönberger , and Perez averaged almost 9 percent before easing to around 5 percent near the summit. All in all, the race 's queen stage had a total of 4,630 metres (15,190 ft) of vertical climbing.\n",
            "\n",
            "As soon as the race 's queen stage started, Neilson Powless (EF Education–EasyPost) attacked out of of the peloton. Michael Matthews was soon joined by five other riders as of the peloton allowed the break 's to gain two minutes. On the climb , Oliveira , Goossens , Schönberger , and Perez of Col du Galibier, attacks flew out of of the peloton as several riders tried to bridge to the break 's. Four riders, Giulio Ciccone (Trek–Segafredo), Louis Meintjes (Intermarché–Wanty–Gobert Matériaux), Tom Pidcock (Ineos Grenadiers), and Chris Froome (Israel–Premier Tech), eventually managed to break away from of the peloton as Team Jumbo–Visma began to set a steady tempo in of the peloton. Meanwhile, up front, Anthony Anthony Perez -LRB- Cofidis -RRB- (Cofidis) left Michael Matthews breakaway companions to take the maximum KoM points at the top of Croix de Fer. On the descent, Powless, Ciccone, and Meintjes bridged up to Anthony Perez -LRB- Cofidis -RRB- to form the lead group. Sebastian Schönberger (B&B Hotels–KTM), Kobe Goossens (Intermarché–Wanty–Gobert Matériaux), Nelson Oliveira (Movistar Team), Pidcock, and Froome soon made Team Jumbo -- Visma proceeded to block the road and allowed the break 's to extend their advantage to more than 14 minutes , ensuring that the break 's will fight for the race 's queen stage win way to the front group to establish the break 's of nine riders. the lead group extended Team Jumbo -- Visma proceeded to block the road and allowed the break 's to extend their advantage to more than 14 minutes , ensuring that the break 's will fight for the race 's queen stage win lead to seven and a half minutes over of the peloton before Team Jumbo–Visma lifted the pace on Col de la Croix de Fer on Col de la Col de la Croix de Fer. On the way to the top of the climb , Oliveira , Goossens , Schönberger , and Perez, Oliveira, Goossens, Schönberger, and Anthony Perez -LRB- Cofidis -RRB- were dropped from the break 's, with the lead quintet crossing the top of Col de la Croix de Fer at almost four and a half minutes before of the peloton did so.\n",
            "\n",
            "On the descent, of the peloton eased the pace on Col de la Croix de Fer, allowing the break 's to extend Team Jumbo -- Visma proceeded to block the road and allowed the break 's to extend their advantage to more than 14 minutes , ensuring that the break 's will fight for the race 's queen stage win advantage to six and a half minutes before Team Jumbo -- Visma proceeded to block the road and allowed the break 's to extend their advantage to more than 14 minutes , ensuring that the break 's will fight for the race 's queen stage win reached the climb , Oliveira , Goossens , Schönberger , and Perez of Alpe d'Huez. With around 10.6 kilometres (6.6 mi) left, Pidcock attacked from the break 's. Initially, Meintjes, as well as Froome a few seconds behind Pogačar, set a steady tempo to keep Pidcock at less than ten seconds ahead but Pidcock gradually extended Michael Matthews lead all the way to the top. Michael Matthews soloed to the top to take Michael Matthews first World Tour win and Pidcock also became the youngest rider to win on Alpe d'Huez. Meintjes took second, 48 seconds down, while Froome finished third at over two minutes behind. Meanwhile, in of the peloton, Team Jumbo–Visma increased the pace on Col de la Croix de Fer on the climb , Oliveira , Goossens , Schönberger , and Perez. The GC contenders began to drop from the lead group until only the maillot jaune, Jonas Vingegaard (Team Jumbo–Visma), Michael Matthews teammate, Sepp Kuss, Tadej Pogačar (UAE Team Emirates), Geraint Geraint Thomas -LRB- Ineos Grenadiers -RRB- (Ineos Grenadiers), and Enric Mas (Movistar Team) remained. Towards the top, Pogačar attacked twice and only Vingegaard was able to follow Pogačar both times. Pogačar and Vingegaard eventually slowed near another summit finish, allowing Geraint Thomas -LRB- Ineos Grenadiers -RRB-, Kuss, and Mas to come back to the sprinters '. At another summit finish, Pogačar sprinted for time gaps but there were no gaps between Pogačar, Vingegaard, and Geraint Thomas -LRB- Ineos Grenadiers -RRB-.\n",
            "\n",
            "In the GC, Vingegaard kept the maillot jaune, 2' 22\" ahead of Pogačar, who moved up to second, while Geraint Thomas -LRB- Ineos Grenadiers -RRB- rose to third, a further four seconds behind. Romain Bardet (Team DSM) dropped to fourth after losing 19 seconds. Adam Yates (Ineos Grenadiers), Nairo Quintana (Arkéa–Samsic), and David Gaudu (Groupama–FDJ) occupy fifth to seventh after having lost between 38 seconds to 1' 21\". Pidcock returned to the top ten with Michael Matthews stage win and time gains on the race 's queen stage.[3][4]\n",
            "\n",
            "the 13th stage of The 2022 Tour de France took the Chemin des Aqueducs into Vienne just before crossing the Rhône and beginning the climb , Oliveira , Goossens , Schönberger , and Perez of Côte Saint-Romain-en-Gal\n",
            "\n",
            "The thirteenth stage featured a transition stage that took the riders from Le Bourg-d'Oisans to Saint-Étienne. From the start, the riders gradually descended towards the third-category Côte de Brié. Following the descent and a long flat section, the riders tackled the second-category Col de Parménie. Another descent and a short flat section led to the intermediate sprint in La Côte-Saint-André with 91 kilometres (57 mi) left. Following a gradual descent, the riders climbed up the final categorised climb of the day, the third-category Côte de Saint-Romain-en-Gal, which was crested with 44 kilometres (27 mi) to go. Near another summit finish, there was an uncategorised climb that peaked with 7.6 kilometres (4.7 mi) remaining. After cresting the top, the riders rode on a plateau section that led to another summit finish in Saint-Étienne.\n",
            "\n",
            "There was a furious fight for the break 's as several riders attempted to go in the move. a furious fight for the break 's took until the climb , Oliveira , Goossens , Schönberger , and Perez of Côte de Brié before a trio of riders, Filippo Ganna (Ineos Grenadiers), Stefan Küng (Groupama–FDJ), and Matteo Jorgenson (Movistar Team), built a gap over of the peloton. Behind the sprinters ', a group of 19 riders tried to bridge up to the trio but Team Jumbo -- Visma proceeded to block the road and allowed the break 's to extend their advantage to more than 14 minutes , ensuring that the break 's will fight for the race 's queen stage win were brought back by the Alpecin–Deceuninck-led peloton. Eventually, Hugo Houle (Israel–Premier Tech), Fred Wright (Team Bahrain Victorious), and the Trek–Segafredo duo of Mads Mads Pedersen and Quinn Quinn Simmons escaped from of the peloton and bridged up to the trio up front. the break 's of seven built a lead of around two and a half minutes as the sprinters' teams kept the sprinters ' in check. At the intermediate sprint, after the break 's took maximum points, Wout van Aert (Team Jumbo–Visma) led of the peloton across to increase Michael Matthews lead in the points classification. With around 71 kilometres (44 mi) left, there was a crash that involved Caleb Ewan (Lotto–Soudal), hurting Michael Matthews knee in the process, but Pidcock continued the race 's. Ahead of the Côte de Saint-Romain-en-Gal, a gap over the peloton started to come down to the break 's.\n",
            "\n",
            "On the climb , Oliveira , Goossens , Schönberger , and Perez, several sprinters began to drop, forcing the sprinters' teams to get off the front of of the peloton. As the pace on Col de la Croix de Fer eased, the break 's's lead increased towards three and a half minutes. With 46 kilometres (29 mi) remaining, Quinn Simmons dropped out of the break 's after finishing Michael Matthews work for Mads Pedersen. Eventually, Team BikeExchange–Jayco took up the chase in of the peloton. Team BikeExchange quickly decreased the break 's's advantage to two and a half minutes before a gap over the peloton stabilised. the break 's 's lead remained at less than two and a half minutes before Team BikeExchange–Jayco called off the chase. Up front, the break 's continued to work well together until Mads Pedersen launched an attack with 10.6 kilometres (6.6 mi) to go. Only Only Wright and Houle were able to follow Pogačar, with a gap over the peloton over Ganna, Küng, and Jorgenson gradually increasing towards another summit finish. Only Wright and Houle attempted to attack Mads Pedersen in the final kilometres, but the Dane was able to respond each time. Only Wright and Houle came to another summit finish together, with Mads Pedersen launching Michael Matthews sprint with 250 metres (820 ft) to go. Only Wright and Houle never came close as Mads Pedersen won Michael Matthews first Grand Tour stage. Only Wright and Houle finished half a minute down while van Aert led of the peloton across almost six minutes down. There were no changes in the top ten as Jonas Vingegaard (Team Jumbo–Visma) kept the maillot jaune.[6][7]\n",
            "\n",
            "The fourteenth stage featured another hilly stage that took the riders from Saint-Étienne to Mende. The first 66.1 kilometres (41.1 mi) had an undulating terrain, with the riders tackling the third-category climbs of Côte de Saint-Just-Malmont and Côte de Châtaignier in the first 40 kilometres (25 mi) as well as the intermediate sprint in Yssingeaux after 50.7 kilometres (31.5 mi) of racing. Following a short flat section, the riders went up an uncategorised climb before tackling the third-category Côte de Grandrieu. Afterwards, the riders rode on a plateau section before going up the third-category Côte de la Fage, which was crested with 30.4 kilometres (18.9 mi) left. Following a long descent, the riders tackled the final categorised climb, the second-category Côte de la Croix Neuve Montée Jalabert, which is 3 kilometres (1.9 mi) long with an average of 10.2 percent. After cresting the top, there were 1.5 kilometres (0.93 mi) left until another summit finish at the Mende Airfield.\n",
            "\n",
            "With the high likelihood of a breakaway challenging for the race 's queen stage win, several riders attempted to make the race 's queen stage into the break 's. With around 180 kilometres (110 mi) still to go, on the Côte de Saint-Just-Malmont, Tadej Pogačar (UAE Team Emirates) attacked twice but the maillot jaune, Jonas Vingegaard (Team Jumbo–Visma), was able to respond both times. The breakaway fight continued on until the riders reached the Côte de Châtaignier with around 153 kilometres (95 mi) left when a group of 23 riders broke away from of the peloton. Team Jumbo–Visma proceeded to block the road and allowed the break 's to extend Team Jumbo -- Visma proceeded to block the road and allowed the break 's to extend their advantage to more than 14 minutes , ensuring that the break 's will fight for the race 's queen stage win advantage to more than 14 minutes, ensuring that the break 's will fight for the race 's queen stage win. the break 's continued to work well together until Michael Matthews (Team BikeExchange–Jayco) attacked with 53.8 kilometres (33.4 mi) remaining. Michael Matthews built a gap over Michael Matthews breakaway companions, who began to attack each other a\n"
          ]
        }
      ],
      "source": [
        "# 📝 TODO: Display text with resolved co-references for the any document of your choice\n",
        "print(docs[0].resolved_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIQZdbp9Gg6v"
      },
      "source": [
        "## Step 5: Disambiguate the entities with Wikidata using OpenTapioca\n",
        "We will use [OpenTapioca](https://opentapioca.org/) to disambiguate the entities with Wikidata and retrieve their unique identifiers (QIDs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUkHtwaqTDTQ"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# Define the API endpoint URL\n",
        "opentapioca_url = 'https://opentapioca.wordlift.io/api/annotate'\n",
        "\n",
        "def opentapioca_annotate(text, language):\n",
        "  # Define the request parameters\n",
        "  params = {\n",
        "    'query': text,\n",
        "    'lang': language[:2]\n",
        "  }\n",
        "\n",
        "  # Send the GET request to the OpenTapioca API endpoint\n",
        "  response = requests.get(opentapioca_url, params=params)\n",
        "\n",
        "  # 📝 TODO: Extract the entities from the API response object\n",
        "  # 💡 You can start by printing the `response` object to understand its structure.\n",
        "\n",
        "  # Convert response to json\n",
        "  j = json.loads(response.text)\n",
        "\n",
        "  # Save only the annotations that have a best_qid\n",
        "  entities = {}\n",
        "  for annotation in j['annotations']:\n",
        "    start = annotation['start']\n",
        "    end = annotation['end']\n",
        "    if annotation['best_qid'] != None:\n",
        "      entities[j['text'][start:end]] = annotation['best_qid']\n",
        "\n",
        "  # Return entities\n",
        "  return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v2QQmase3CD"
      },
      "outputs": [],
      "source": [
        "for doc in docs:\n",
        "  doc.wiki_entities = {}\n",
        "  entities = {}\n",
        "  for j in range(0, len(doc.raw_text), 4000):\n",
        "    doc.wiki_entities |= opentapioca_annotate(doc.raw_text[j:j+4000], doc.language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cC1zoDSe5BF"
      },
      "source": [
        "Display extracted Wikidata entities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B80_ZKM7SoIF"
      },
      "outputs": [],
      "source": [
        "# 📝 TODO: Display extracted Wikidata entities for the first document\n",
        "for entity in docs[0].wiki_entities.items():\n",
        "    print(\"\\\"{}\\\" -> \\\"{}\\\"\".format(entity[0], entity[1]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNvv056SGirj"
      },
      "source": [
        "## Step 6: Run relation extraction using Stanford OpenIE\n",
        "We will use Stanford OpenIE to extract the relations between the entities in the input text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVVKYu3CGjaC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "# Create a StanfordCoreNLP object\n",
        "nlp = StanfordCoreNLP('https://websem:eurecom@corenlp.tools.eurecom.fr')\n",
        "\n",
        "# Define a function to extract relations from input text using Stanford OpenIE\n",
        "def extract_relations(input_text, language):\n",
        "  output = nlp.annotate(input_text, properties={\n",
        "    'timeout': 300000,\n",
        "    'annotators': 'tokenize,ssplit,openie',\n",
        "    'outputFormat': 'json',\n",
        "    'pipelineLanguage': language[:2]\n",
        "  })\n",
        "  try:\n",
        "    output = json.loads(output)\n",
        "  except Exception as err:\n",
        "    print(f'Unexpected response: {output}')\n",
        "    raise\n",
        "\n",
        "  # 📝 TODO: Get relations from the `output` object (subject, relation, object)\n",
        "  #    and append them to a `relations` list.\n",
        "  # 💡 You can start by printing the `output` object to understand its structure.\n",
        "\n",
        "  relations = []\n",
        "  if 'sentences' in output:\n",
        "      # Iterate over sentences\n",
        "      for sentence in output['sentences']:\n",
        "        # Check if 'openie' key has been found\n",
        "        if 'openie' in sentence:\n",
        "          # Iterate over openie entries\n",
        "          for openie_entry in sentence['openie']:\n",
        "            # Append (subject, relation, object) to list\n",
        "            relations.append((openie_entry['subject'], openie_entry['relation'], openie_entry['object']))\n",
        "\n",
        "  # Return relations\n",
        "  return relations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wa4T96f1f2Yd"
      },
      "outputs": [],
      "source": [
        "for doc in docs:\n",
        "  if doc.language == \"english\":  # CoreNLP OpenIE only supports english\n",
        "    doc.relations = extract_relations(doc.raw_text, doc.language)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSSohZ89_7Xk"
      },
      "source": [
        "Display relations which have been extracted:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI5oEP91-l66"
      },
      "outputs": [],
      "source": [
        "# 📝 TODO: Display extracted relations for the first document\n",
        "print(docs[0].relations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_YdNyfHGlC1"
      },
      "source": [
        "## Step 7: Implement some mappings between the entity types and relations returned with a given cycling ontology\n",
        "We will implement mappings between the entity types and relations returned with the cycling ontology available at https://www.eurecom.fr/~troncy/teaching/websem2023/cycling.owl."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwo01wgOGmMc",
        "outputId": "8f767d18-bfdd-4316-9f5b-2438b7d5bff7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Graph identifier=https://raw.githubusercontent.com/efrenbg1/WebSem/main/Lab%203/cycling.owl (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import rdflib\n",
        "\n",
        "s = rdflib.URIRef(\"https://raw.githubusercontent.com/efrenbg1/WebSem/main/Lab%203/cycling.owl\")\n",
        "g = rdflib.Graph(identifier=s)\n",
        "\n",
        "# 📝 TODO: Create an RDF graph based on the cycling ontology and using the data\n",
        "#    from `relations_en`, `entities_en`, and `wiki_entities_en`.\n",
        "\n",
        "# Load the cycling ontology from the given URI\n",
        "ontology_uri = \"https://raw.githubusercontent.com/efrenbg1/WebSem/main/Lab%203/cycling.owl\"\n",
        "g.parse(ontology_uri, format=\"xml\")\n",
        "\n",
        "# Assuming `relations_en`, `entities_en`, and `wiki_entities_en` are available datasets\n",
        "# You should replace the placeholder paths with the actual paths to your datasets.\n",
        "relations_path = ontology_uri+\"#relations_en\"\n",
        "entities_path = ontology_uri+\"#entities_en\"\n",
        "wiki_entities_path = ontology_uri+\"#wiki_entities_en\"\n",
        "\n",
        "# Load the datasets into the RDF graph\n",
        "g.parse(relations_path, format=\"xml\")\n",
        "g.parse(entities_path, format=\"xml\")\n",
        "g.parse(wiki_entities_path, format=\"xml\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nq4jHBEsUxCo",
        "outputId": "ed364bc9-4785-44a3-e932-a99ce78963ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Graph identifier=https://raw.githubusercontent.com/efrenbg1/WebSem/main/Lab%203/cycling.owl (<class 'rdflib.graph.Graph'>)>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the result into a file\n",
        "g.serialize(destination='output.ttl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJXcKJUZGmqM"
      },
      "source": [
        "## Step 8: Load the data in the Corese engine with the ontology and write the SPARQL queries to retrieve specific information from the KG\n",
        "We will load the data in the [Corese](https://www.eurecom.fr/~troncy/teaching/websem2023/corese-3.2.3c.jar) engine (the same you used in the Assignment 2) with the ontology and write the SPARQL queries to retrieve specific information from the KG. We will write the following queries:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSLn1iZB3BUR"
      },
      "source": [
        "* 📝 List the name of the cycling teams\n",
        "\n",
        "PREFIX cycling: <https://purl.org/websem/cycling#>\n",
        "\n",
        "SELECT ?team ?teamName\n",
        "WHERE {\n",
        "  ?team rdf:type cycling:Team .\n",
        "  OPTIONAL { ?team cycling:name ?teamName }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0ITJ6LO3C9J"
      },
      "source": [
        "* 📝 List the name of the cycling riders\n",
        "\n",
        "PREFIX cycling: <https://purl.org/websem/cycling#>\n",
        "\n",
        "SELECT DISTINCT ?riderName\n",
        "WHERE {\n",
        "  ?rider rdf:type cycling:Rider .\n",
        "  ?rider cycling:name ?riderName .\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5u5FyOq3FjB"
      },
      "source": [
        "* 📝 Retrieve the name of the winner of the Prologue\n",
        "\n",
        "PREFIX cycling: <https://purl.org/websem/cycling#>\n",
        "\n",
        "\n",
        "SELECT DISTINCT ?winnerName\n",
        "WHERE {\n",
        "  ?prologue rdf:type cycling:Prologue .\n",
        "  ?prologue cycling:isWinnerOf ?winnerStage .\n",
        "  ?winnerStage rdf:type cycling:Stage .\n",
        "  ?winnerStage cycling:composedOf ?winnerRider .\n",
        "  ?winnerRider rdf:type cycling:Rider .\n",
        "  ?winnerRider cycling:name ?winnerName .\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRpBPPYR3HTi"
      },
      "source": [
        "📝 We will also write the same 3 queries on Wikidata starting from `Q98043180` to compare the results."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}