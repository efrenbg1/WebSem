{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# WebSem Project: Constructing and Querying a Knowledge Graph in the Cycling Domain\n",
        "\n",
        "## Introduction\n",
        "\n",
        "The goal of this project is to extract information from multilingual textual documents about cycling and create a knowledge graph (KG) using the extracted entities and relations. The KG will be compatible with a cycling ontology and queries will be written in SPARQL to retrieve specific information from the KG. The project will be implemented using Jupyter Notebook and the following steps will be followed:\n",
        "\n",
        "* Collect multilingual textual documents about cycling.\n",
        "* Pre-process the documents to get clean text files.\n",
        "* Run named entity recognition (NER) on the documents to extract named entities of the type Person, Organization and Location using spaCy.\n",
        "* Run co-reference resolution on the input text using spaCy.\n",
        "* Disambiguate the entities with Wikidata using OpenTapioca.\n",
        "* Run relation extraction using Stanford OpenIE.\n",
        "* Implement some mappings between the entity types and relations returned with the cycling ontology you developed during the Assignment 1 in order to create a knowledge graph of the domain represented in RDF.\n",
        "* Load the data in the Corese engine as you did for the Assignment 2 with your cycling ontology and the knowledge graph built in the previous step and write some SPARQL queries to retrieve specific information from the KG.\n",
        "\n",
        "### Useful resources\n",
        "* The github repository \"Building knowledge graph from input data\" at  https://github.com/varun196/knowledge_graph_from_unstructured_text can be used as an inspiration.\n",
        "\n",
        "### References\n",
        "* NLTK: https://www.nltk.org/\n",
        "* spaCy: https://spacy.io/\n",
        "* Stanford OpenIE: https://nlp.stanford.edu/software/openie.html\n",
        "* OpenTapioca: https://opentapioca.org/\n",
        "* Corese engine: https://project.inria.fr/corese/\n",
        "* Wikidata: https://www.wikidata.org/"
      ],
      "metadata": {
        "id": "mpecDDB_GGLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Collect multilingual textual documents about cycling\n",
        "For this mini project, we will collect multilingual textual documents about cycling from various sources such as news articles, blog posts, and race reports. We will download the documents and save them in a directory called `cycling_docs`.\n",
        "\n",
        "The list of documents to download are available at:\n",
        "\n",
        "* English:\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11\n",
        " - https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21\n",
        " - https://www.bbc.com/sport/cycling/61940037\n",
        " - https://www.bbc.com/sport/cycling/62017114 (stage 1)\n",
        " - https://www.bbc.com/sport/cycling/62097721 (stage 7)\n",
        " - https://www.bbc.com/sport/cycling/62153759 (stage 11)\n",
        " - https://www.bbc.co.uk/sport/cycling/62285420 (stage 21)\n",
        "\n",
        "* French:\n",
        " - https://fr.wikipedia.org/wiki/Tour_de_France_2022\n",
        " - https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html\n",
        " - https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html"
      ],
      "metadata": {
        "id": "EzMinLFxGUFx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# Feel free to install more dependencies if needed!\n",
        "#\n",
        "\n",
        "# Install jusText for automatically extracting text from web pages\n",
        "!pip install --quiet jusText\n",
        "\n",
        "# Install nltk for text processing\n",
        "!pip install --quiet nltk\n",
        "\n",
        "# Install spaCy for NER extraction\n",
        "!pip install --quiet spacy\n",
        "\n",
        "# Install pycorenlp for Stanford CoreNLP\n",
        "!pip install --quiet pycorenlp\n",
        "\n",
        "# Install pandas for data visualization\n",
        "!pip install --quiet pandas\n",
        "\n",
        "# Install rdflib for writing RDF\n",
        "!pip install --quiet rdflib"
      ],
      "metadata": {
        "id": "HccrPk8uGVz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "import requests\n",
        "import justext\n",
        "import os\n",
        "from urllib.parse import urlsplit\n",
        "\n",
        "\n",
        "# Define a function to get filename from URL\n",
        "def get_filename_from_url(url):\n",
        "  urlpath = urlsplit(url).path\n",
        "  return os.path.basename(urlpath)\n",
        "\n",
        "\n",
        "# Define a function to download URLs and extract text\n",
        "def download_urls(urls_list, language):\n",
        "  # Loop over each URL in the list\n",
        "  for url in urls_list:\n",
        "    # Fetch and extract text from the URL using jusText\n",
        "    response = requests.get(url)\n",
        "    paragraphs = justext.justext(\n",
        "      response.content,\n",
        "      justext.get_stoplist(language.capitalize()),\n",
        "      no_headings=True,\n",
        "      max_heading_distance=150,\n",
        "      length_low=70,\n",
        "      length_high=140,\n",
        "      stopwords_low=0.2,\n",
        "      stopwords_high=0.3,\n",
        "      max_link_density=0.4\n",
        "    )\n",
        "    extracted_text = '\\n'.join(list(filter(None, map(\n",
        "      lambda paragraph: paragraph.text if not paragraph.is_boilerplate else '',\n",
        "      paragraphs\n",
        "    ))))\n",
        "\n",
        "    # Truncate text if it's too long\n",
        "    extracted_text = extracted_text[0:10000]\n",
        "\n",
        "    # Create the output directory if it does not exist\n",
        "    output_dir = os.path.join('cycling_docs', language)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Save extracted text as a .txt file\n",
        "    filename = get_filename_from_url(url)\n",
        "    output_path = os.path.join(output_dir, f'{filename}.txt')\n",
        "    with open(output_path, 'w') as f:\n",
        "      f.write(extracted_text)\n",
        "\n",
        "    print(f'Downloaded {url} into {output_path}')\n",
        "\n",
        "\n",
        "# List of URLs to download\n",
        "urls_list_english = [\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France',\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_1_to_Stage_11',\n",
        "  'https://en.wikipedia.org/wiki/2022_Tour_de_France,_Stage_12_to_Stage_21',\n",
        "  'https://www.bbc.com/sport/cycling/61940037',\n",
        "  'https://www.bbc.com/sport/cycling/62017114',\n",
        "  'https://www.bbc.com/sport/cycling/62097721',\n",
        "  'https://www.bbc.com/sport/cycling/62153759',\n",
        "  'https://www.bbc.co.uk/sport/cycling/62285420',\n",
        "]\n",
        "urls_list_french = [\n",
        "  'https://fr.wikipedia.org/wiki/Tour_de_France_2022',\n",
        "  'https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-epoustouflant-jonas-vingegaard-remporte-la-11e-etape-et-s-empare-du-maillot-jaune-de-tadej-pogacar_5254102.html',\n",
        "  'https://www.francetvinfo.fr/tour-de-france/tour-de-france-2022-jonas-vingegaard-vainqueur-de-sa-premiere-grande-boucle-jasper-philipsen-s-offre-au-sprint-la-21e-etape_5275612.html',\n",
        "]\n",
        "\n",
        "# Download the listed URLs\n",
        "download_urls(urls_list_english, 'english')\n",
        "download_urls(urls_list_french, 'french')"
      ],
      "metadata": {
        "id": "_031XjozIHqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Pre-process the documents to get clean txt files\n",
        "We will pre-process the documents to get clean txt files by removing any unnecessary characters, punctuation, and stopwords. We will use Python's [re](https://docs.python.org/3/library/re.html) and [nltk](https://www.nltk.org/) libraries for this purpose. We will save the results in a `clean_docs` folder."
      ],
      "metadata": {
        "id": "Wg52A5ELGWvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Document class which holds all the necessary variables for the purpose of this\n",
        "project.\n",
        "\"\"\"\n",
        "class Document:\n",
        "  def __init__(self, text, language = None, raw_text = None, filepath = None):\n",
        "    self.language = language   # Language of the document\n",
        "    self.raw_text = raw_text   # Origial text before cleaning\n",
        "    self.text = text           # Text after cleaning\n",
        "    self.resolved_text = None  # Text after resolving co-references\n",
        "    self.filepath = filepath   # Path to the document file\n",
        "    self.spacy_entities = []   # List of spaCy entities\n",
        "    self.coreferences = None   # CoreNLP coreferences object\n",
        "    self.wiki_entities = {}    # Dictionary of Wikidata entities\n",
        "    self.relations = []        # List of OpenIE relations"
      ],
      "metadata": {
        "id": "t9sdmals1W7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📝 TODO: Import the necessary libraries for natural language processing\n",
        "import os\n",
        "\n",
        "def clean_text(dirty_text, language):\n",
        "  # 📝 TODO: Define a function to clean text (words tokenization, stopwords\n",
        "  #          removal, ...).\n",
        "  # `cleaned_text = ...`\n",
        "\n",
        "  # Return the cleaned text\n",
        "  return cleaned_text"
      ],
      "metadata": {
        "id": "k9JsLAGsxsyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to process a file and write the result to a new file\n",
        "def process_file(file, language):\n",
        "  # Open the file in read-only mode and read all of its lines\n",
        "  with open(file, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "  # Concatenate all the lines into a single string\n",
        "  raw_text = '\\n'.join(lines)\n",
        "\n",
        "  # Clean the text using the `clean_text` function\n",
        "  cleaned_text = clean_text(raw_text, language)\n",
        "\n",
        "  # Create a new document and return it\n",
        "  doc = Document(cleaned_text, language=language, raw_text=raw_text, filepath=os.path.abspath(file))\n",
        "  return doc\n",
        "\n",
        "\n",
        "# Create a list to store all our documents\n",
        "docs = []\n",
        "\n",
        "# Loop through all the files in the \"cycling_docs\" folder\n",
        "folder = 'cycling_docs'\n",
        "for language in os.listdir(folder):\n",
        "  for filename in os.listdir(os.path.join(folder, language)):\n",
        "    # Construct the full path to the file\n",
        "    file = os.path.join(folder, language, filename)\n",
        "\n",
        "    # Check if the file is a regular file and has a .txt extension\n",
        "    if os.path.isfile(file) and file.endswith('.txt'):\n",
        "      # Process the file and append the new document to our list\n",
        "      doc = process_file(file, language)\n",
        "      docs.append(doc)"
      ],
      "metadata": {
        "id": "KhFqE5TleLz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the text of the first document\n",
        "display(docs[0].text)"
      ],
      "metadata": {
        "id": "yi3wcREO1plh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Run named entity recognition (NER) on the documents\n",
        "We will use [spaCy](https://spacy.io)'s pre-trained models to perform NER on the documents and extract the entities of type PER/ORG/LOC. We will save the extracted entities in a file."
      ],
      "metadata": {
        "id": "nXH__c_rGaIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📝 TODO: Import spaCy and other libraries that might be required for entity\n",
        "#          extraction\n",
        "\n",
        "def extract_entities(text, language):\n",
        "  # 📝 TODO: Use spaCy to extract named entities and store them into a list.\n",
        "  # The format of the end result should look like this:\n",
        "  # ```\n",
        "  # entities = [\n",
        "  #   { \"text\": \"Tour de France\", \"label\": \"ORG\" },\n",
        "  #   { \"text\": \"Peter Sagan\", \"label\": \"PERSON\" },\n",
        "  # ]\n",
        "  # ```\n",
        "\n",
        "  # 📝\n",
        "\n",
        "  # Return extracted entities\n",
        "  return entities"
      ],
      "metadata": {
        "id": "MkMYXqVFGy9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract entities for each document\n",
        "for doc in docs:\n",
        "  doc.spacy_entities = extract_entities(doc.text, doc.language)"
      ],
      "metadata": {
        "id": "REq50-k7ePSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display entities which have been extracted:"
      ],
      "metadata": {
        "id": "NkFvU6OfDwux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📝 TODO: Display the extracted entities for the first document"
      ],
      "metadata": {
        "id": "ER2xsq4WYJgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: Run co-reference resolution on the input text\n",
        "We will use CoreNLP to perform [co-reference resolution](https://en.wikipedia.org/wiki/Coreference) on the input text and resolve coreferences.\n",
        "\n",
        "For this project, we will use a hosted version of CoreNLP at: https://corenlp.tools.eurecom.fr/ (username: `websem`, password: `eurecom`). Feel free to try out the web interface before writing the code.\n",
        "\n",
        "First, we compute the annotations and store them into the `coreferences` variable of our Document:"
      ],
      "metadata": {
        "id": "lAsxblm4GceJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "\n",
        "# Set up the CoreNLP client\n",
        "nlp = StanfordCoreNLP('https://websem:eurecom@corenlp.tools.eurecom.fr')\n",
        "\n",
        "# Define a function which computes coreferences for a given text and language\n",
        "def compute_coreferences(text, language):\n",
        "  props = {\n",
        "    'timeout': 300000,\n",
        "    'annotators': 'tokenize,ssplit,coref',\n",
        "    'pipelineLanguage': language[:2],\n",
        "    'outputFormat': 'json'\n",
        "  }\n",
        "\n",
        "  # Annotate the text for co-reference resolution\n",
        "  corenlp_output = nlp.annotate(text, properties=props)\n",
        "  try:\n",
        "    corenlp_output = json.loads(corenlp_output)\n",
        "  except Exception as err:\n",
        "    print(f'Unexpected response: {corenlp_output}')\n",
        "    raise\n",
        "\n",
        "  return corenlp_output"
      ],
      "metadata": {
        "id": "v0WOTNW3Ggg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test co-references computation\n",
        "example = compute_coreferences(\"John is a software engineer. He is very talented. Sarah is a designer. She works with him.\", language=\"en\")\n",
        "\n",
        "# Pretty-print them\n",
        "print(json.dumps(example, indent=2))"
      ],
      "metadata": {
        "id": "ajQ2T6QyhWop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute co-references for all documents\n",
        "for doc in docs:\n",
        "  if doc.language == \"english\":  # CoreNLP Coref-resolution only supports english\n",
        "    doc.coreferences = compute_coreferences(doc.raw_text, doc.language)"
      ],
      "metadata": {
        "id": "FSnVwbZkhUii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is to display all co-references for each mentions in the text.\n",
        "\n",
        "For example:\n",
        "\n",
        "> \"He\" -> \"John\"\n",
        ">\n",
        "> \"She\" -> \"Sarah\"\n",
        ">\n",
        "> \"him\" -> \"John\""
      ],
      "metadata": {
        "id": "iBqJ8DTtoFpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for coref_cluster in example['corefs'].values():\n",
        "  # 📝 TODO: Print each co-references like so: \"He\" -> \"John\"\n",
        "  # 💡 Each cluster has one representative mention, flagged with `isRepresentativeMention: True`"
      ],
      "metadata": {
        "id": "vp2BHxDHoSUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 🏆 Challenge\n",
        "\n",
        "Replace values within the text with their resolved co-reference. For example, with the following text:\n",
        "\n",
        "> **John** is a software engineer. **He** is very talented.\n",
        "\n",
        "In the second sentence, the pronoun \"He\" would be replaced with its co-reference, and the final text would become:\n",
        "\n",
        "> **John** is a software engineer. **John** is very talented."
      ],
      "metadata": {
        "id": "bVhTqao-5Z55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function which resolves coreferences inside a document\n",
        "def resolve_coreferences(doc):\n",
        "  corenlp_output = doc.coreferences\n",
        "\n",
        "  resolved_text = \"\"\n",
        "\n",
        "  # 📝 TODO: Replace values within the text with their resolved co-reference.\n",
        "  # 💡 You can start by printing the `corenlp_output` object to understand its\n",
        "  #    structure.\n",
        "\n",
        "\n",
        "  return resolved_text"
      ],
      "metadata": {
        "id": "V4P_mgqCeebD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test resolving co-references\n",
        "original_text = \"John is a software engineer. He is very talented. Sarah is a designer. She works with him.\"\n",
        "corefs = compute_coreferences(original_text, language=\"en\")\n",
        "resolved_text = resolve_coreferences(corefs)\n",
        "print(original_text)\n",
        "print(resolved_text)"
      ],
      "metadata": {
        "id": "5rXXcAtMklMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resolve co-references for all documents\n",
        "for doc in docs:\n",
        "  if doc.coreferences is not None:\n",
        "    doc.resolved_text = resolve_coreferences(doc.text)"
      ],
      "metadata": {
        "id": "ntDsoAUikd4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 📝 TODO: Display text with resolved co-references for the any document of your choice"
      ],
      "metadata": {
        "id": "hINiwG5V-__O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Disambiguate the entities with Wikidata using OpenTapioca\n",
        "We will use [OpenTapioca](https://opentapioca.org/) to disambiguate the entities with Wikidata and retrieve their unique identifiers (QIDs)."
      ],
      "metadata": {
        "id": "oIQZdbp9Gg6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Define the API endpoint URL\n",
        "opentapioca_url = 'https://opentapioca.wordlift.io/api/annotate'\n",
        "\n",
        "def opentapioca_annotate(text, language):\n",
        "  # Define the request parameters\n",
        "  params = {\n",
        "    'query': text,\n",
        "    'lang': language[:2]\n",
        "  }\n",
        "\n",
        "  # Send the GET request to the OpenTapioca API endpoint\n",
        "  response = requests.get(opentapioca_url, params=params)\n",
        "\n",
        "  # 📝 TODO: Extract the entities from the API response object\n",
        "  # 💡 You can start by printing the `response` object to understand its structure.\n",
        "\n",
        "  # Return entities\n",
        "  return entities"
      ],
      "metadata": {
        "id": "CUkHtwaqTDTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  doc.wiki_entities = {}\n",
        "  entities = {}\n",
        "  for j in range(0, len(doc.text), 4000):\n",
        "    doc.wiki_entities |= opentapioca_annotate(doc.text[j:j+4000], doc.language)"
      ],
      "metadata": {
        "id": "3v2QQmase3CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display extracted Wikidata entities:"
      ],
      "metadata": {
        "id": "5cC1zoDSe5BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📝 TODO: Display extracted Wikidata entities for the first document"
      ],
      "metadata": {
        "id": "B80_ZKM7SoIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: Run relation extraction using Stanford OpenIE\n",
        "We will use Stanford OpenIE to extract the relations between the entities in the input text."
      ],
      "metadata": {
        "id": "fNvv056SGirj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pycorenlp import StanfordCoreNLP\n",
        "\n",
        "# Create a StanfordCoreNLP object\n",
        "nlp = StanfordCoreNLP('https://websem:eurecom@corenlp.tools.eurecom.fr')\n",
        "\n",
        "# Define a function to extract relations from input text using Stanford OpenIE\n",
        "def extract_relations(input_text, language):\n",
        "  output = nlp.annotate(input_text, properties={\n",
        "    'timeout': 300000,\n",
        "    'annotators': 'tokenize,ssplit,openie',\n",
        "    'outputFormat': 'json',\n",
        "    'pipelineLanguage': language[:2]\n",
        "  })\n",
        "  try:\n",
        "    output = json.loads(output)\n",
        "  except Exception as err:\n",
        "    print(f'Unexpected response: {output}')\n",
        "    raise\n",
        "\n",
        "  # 📝 TODO: Get relations from the `output` object (subject, relation, object)\n",
        "  #    and append them to a `relations` list.\n",
        "  # 💡 You can start by printing the `output` object to understand its structure.\n",
        "\n",
        "  # Return relations\n",
        "  return relations"
      ],
      "metadata": {
        "id": "mVVKYu3CGjaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in docs:\n",
        "  if doc.language == \"english\":  # CoreNLP OpenIE only supports english\n",
        "    doc.relations = extract_relations(doc.text, doc.language)"
      ],
      "metadata": {
        "id": "Wa4T96f1f2Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display relations which have been extracted:"
      ],
      "metadata": {
        "id": "zSSohZ89_7Xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 📝 TODO: Display extracted relations for the first document"
      ],
      "metadata": {
        "id": "VI5oEP91-l66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Implement some mappings between the entity types and relations returned with a given cycling ontology\n",
        "We will implement mappings between the entity types and relations returned with the cycling ontology available at https://www.eurecom.fr/~troncy/teaching/websem2023/cycling.owl."
      ],
      "metadata": {
        "id": "A_YdNyfHGlC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import rdflib\n",
        "\n",
        "g = Graph()\n",
        "\n",
        "# 📝 TODO: Create an RDF graph based on the cycling ontology and using the data\n",
        "#    from `relations_en`, `entities_en`, and `wiki_entities_en`."
      ],
      "metadata": {
        "id": "uwo01wgOGmMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the result into a file\n",
        "g.serialize(destination='output.ttl')"
      ],
      "metadata": {
        "id": "Nq4jHBEsUxCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 8: Load the data in the Corese engine with the ontology and write the SPARQL queries to retrieve specific information from the KG\n",
        "We will load the data in the [Corese](https://www.eurecom.fr/~troncy/teaching/websem2023/corese-3.2.3c.jar) engine (the same you used in the Assignment 2) with the ontology and write the SPARQL queries to retrieve specific information from the KG. We will write the following queries:"
      ],
      "metadata": {
        "id": "VJXcKJUZGmqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 📝 List the name of the cycling teams"
      ],
      "metadata": {
        "id": "cSLn1iZB3BUR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 📝 List the name of the cycling riders"
      ],
      "metadata": {
        "id": "K0ITJ6LO3C9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 📝 Retrieve the name of the winner of the Prologue"
      ],
      "metadata": {
        "id": "t5u5FyOq3FjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "📝 We will also write the same 3 queries on Wikidata starting from `Q98043180` to compare the results."
      ],
      "metadata": {
        "id": "kRpBPPYR3HTi"
      }
    }
  ]
}